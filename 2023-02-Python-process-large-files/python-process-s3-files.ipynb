{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "from typing import Any, Generator\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from functional import seq  # type: ignore[reportMissingTypeStubs]\n",
    "\n",
    "import boto3  # type: ignore[reportMissingTypeStubs]\n",
    "from botocore import UNSIGNED  # type: ignore[reportMissingTypeStubs]\n",
    "from botocore.client import Config  # type: ignore[reportMissingTypeStubs]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 constants\n",
    "BUCKET_NAME = \"mccarter-blog\"\n",
    "FILE_PREFIX = \"2023-02-18/small-files/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "These are small helper functions.\n",
    "\n",
    "Each function does exactly one task, following the Single Responsibility Principal.\n",
    "\n",
    "We can then easily compose these functions into a simple data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_s3_bucket_file_keys(\n",
    "    s3,  # type: ignore\n",
    "    bucket_name: str,\n",
    "    prefix: str,\n",
    ") -> Generator[str, None, None]:\n",
    "    \"\"\"Returns a generator of file names (technically S3 bucket keys) from the given S3 bucket\n",
    "    that have the given path prefix.\n",
    "\n",
    "    Args:\n",
    "        s3: The boto3 S3 client object\n",
    "        bucket_name (str): The name of the S3 bucket to retrieve data from\n",
    "        prefix (str): The \"file path\" prefix of the files (S3 bucket keys) to be returned.\n",
    "            So all file paths returned from this function will have this prefix.\n",
    "\n",
    "    Yields:\n",
    "        Generator[str, None, None]: A sequence of matching file names (S3 bucket keys).\n",
    "    \"\"\"\n",
    "\n",
    "    # List all objects in the bucket with specified \"file/folder\" prefix\n",
    "    response: dict[str, Any] = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)  # type: ignore\n",
    "\n",
    "    # Iterate through each object in the response\n",
    "    for obj in response.get(\"Contents\", []):\n",
    "        # Get the object key (i.e., file path)\n",
    "        key = obj.get(\"Key\")\n",
    "\n",
    "        # Skip directories - this function only returns file paths\n",
    "        if key.endswith(\"/\"):\n",
    "            continue\n",
    "\n",
    "        yield key\n",
    "\n",
    "\n",
    "# Connect to S3 anonymously - the S3 bucket has permissions set to allow this\n",
    "s3 = boto3.client(\"s3\", config=Config(signature_version=UNSIGNED))  # type: ignore\n",
    "\n",
    "test_keys = list_s3_bucket_file_keys(s3, BUCKET_NAME, FILE_PREFIX)\n",
    "test_results = seq(test_keys).take(3)  # type: ignore\n",
    "expected = [\n",
    "    \"2023-02-18/small-files/black.json.gz\",\n",
    "    \"2023-02-18/small-files/blue.json.gz\",\n",
    "    \"2023-02-18/small-files/gray.json.gz\",\n",
    "]\n",
    "assert test_results == expected, f\"Expected {expected} but got {test_results}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_s3_file(s3, bucket_name: str, key: str) -> bytes:  # type: ignore\n",
    "    \"\"\"Download the given file (S3 bucket key) from S3 and return the raw bytes\n",
    "\n",
    "    Args:\n",
    "        s3: The boto3 S3 client object\n",
    "        bucket_name (str): The name of the S3 bucket to retrieve data from\n",
    "        key (str): The S3 key (file name if you will) of the object to download\n",
    "\n",
    "    Returns:\n",
    "        bytes: The raw bytes of the given file from S3\n",
    "    \"\"\"\n",
    "    obj_response = s3.get_object(Bucket=bucket_name, Key=key)  # type: ignore\n",
    "    content: bytes = obj_response[\"Body\"].read()  # type: ignore\n",
    "    return content\n",
    "\n",
    "\n",
    "# Do something with the content (e.g., print it)\n",
    "content = download_s3_file(s3, BUCKET_NAME, \"2023-02-18/small-files/blue.json.gz\")\n",
    "assert type(content) == bytes, f\"Expected {type(bytes)} but got {type(content)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_binary_file(file_name: str) -> bytes:\n",
    "    \"\"\"A very simple function that reads the byte for the given file name\n",
    "\n",
    "    Args:\n",
    "        file_name (str): The name of the file to read\n",
    "\n",
    "    Returns:\n",
    "        bytes: The raw bytes of the given binary file\n",
    "    \"\"\"\n",
    "    with open(file_name, \"rb\") as file:  # r for reading, b for binary file\n",
    "        return file.read()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our business / domain layer\n",
    "\n",
    "Here we define our simple domain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class ColourId:\n",
    "    \"\"\"A very small data class that holds a colour name and details.\"\"\"\n",
    "\n",
    "    id: str\n",
    "    name: str\n",
    "    base: str\n",
    "    hex: str\n",
    "    rgb: tuple[int, int, int]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some simple questions we can ask of our colour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_colour_reddish(colour: ColourId) -> bool:\n",
    "    return colour.rgb[0] > colour.rgb[1] and colour.rgb[0] > colour.rgb[2]\n",
    "\n",
    "\n",
    "def is_colour_muted(colour: ColourId) -> bool:\n",
    "    return colour.rgb[0] < 128 and colour.rgb[1] < 128 and colour.rgb[2] < 128"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data access layer\n",
    "\n",
    "Our data access layer code is very simle - just a helper function to convert a JSON object\n",
    "into an instance of our `ColourId` domain object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_colour_from_json(data: dict[str, Any]) -> ColourId:\n",
    "    return ColourId(\n",
    "        data[\"id\"],\n",
    "        data[\"name\"],\n",
    "        data[\"base\"],\n",
    "        data[\"hex\"],\n",
    "        data[\"rgb\"],\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data pipeline examples\n",
    "\n",
    "Our first data pipeline just reads some test files from our local drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = [\n",
    "    \"./small-files/purple.json.gz\",\n",
    "    \"./small-files/gray.json.gz\",\n",
    "    \"./small-files/green.json.gz\",\n",
    "]\n",
    "\n",
    "result = (  # type: ignore\n",
    "    seq(test_files)  # type: ignore\n",
    "    .map(read_binary_file)\n",
    "    .map(gzip.decompress)  # type: ignore\n",
    "    .map(json.loads)  # type: ignore\n",
    "    .flatten()  # type: ignore\n",
    "    .to_list()  # type: ignore\n",
    ")\n",
    "\n",
    "# print(json.dumps(result, indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our second data pipeline reads all of our .gz compressed JSON files from the S3 bucket.\n",
    "\n",
    "We:\n",
    "1. Get the list of all the files in our S3 bucket (prefix) folder \n",
    "2. Download the raw bytes of the file\n",
    "3. Decompress the in-memory bytes to get the original .json file contents\n",
    "4. Parse the in-memory JSON data into a Python list of dictionaries\n",
    "5. Convert each dictionary into our Python `ColourId` domain object\n",
    "6. Then we convert the entire thing into an in-memory list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 76 colours\n",
      "ColourId(id='b6907772-f286-4df1-b699-1d9dee8fdbb2', name='Asphalt', base='black', hex='#0C0404', rgb=[12, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "result: list[ColourId] = (  # type: ignore\n",
    "    seq(list_s3_bucket_file_keys(s3, BUCKET_NAME, FILE_PREFIX))  # type: ignore\n",
    "    .map(lambda key: download_s3_file(s3, BUCKET_NAME, key))  # type: ignore\n",
    "    .map(gzip.decompress)  # type: ignore\n",
    "    .map(json.loads)  # type: ignore\n",
    "    .flatten()  # type: ignore\n",
    "    .map(create_colour_from_json)  # type: ignore\n",
    "    .filter(is_colour_reddish)  # type: ignore\n",
    "    .filter(is_colour_muted)  # type: ignore\n",
    "    .to_list()  # type: ignore\n",
    ")\n",
    "\n",
    "print(f\"Found {len(result)} colours\")\n",
    "print(result[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The usual imperative way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 76 colours\n",
      "ColourId(id='b6907772-f286-4df1-b699-1d9dee8fdbb2', name='Asphalt', base='black', hex='#0C0404', rgb=[12, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "result: list[ColourId] = []\n",
    "\n",
    "for s3_key in list_s3_bucket_file_keys(s3, BUCKET_NAME, FILE_PREFIX):  # type: ignore\n",
    "    downloaded_file_bytes = download_s3_file(s3, BUCKET_NAME, s3_key)  # type: ignore\n",
    "    decompressed_text = gzip.decompress(downloaded_file_bytes)\n",
    "    json_data = json.loads(decompressed_text)\n",
    "\n",
    "    for colour_dict in json_data:\n",
    "        colour = create_colour_from_json(colour_dict)\n",
    "\n",
    "        # Apply our filters\n",
    "        if not is_colour_reddish(colour):\n",
    "            continue\n",
    "        if not is_colour_muted(colour):\n",
    "            continue\n",
    "\n",
    "        result.append(colour)\n",
    "\n",
    "print(f\"Found {len(result)} colours\")\n",
    "print(result[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb06f964ad5f6b1bbfd3fdf7106bdbdd9638f61f30f9c922a33f9fa20d941e6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
