{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Generators\n",
    "\n",
    "This Jupyter notebook explores how we can read very large S3 buckets - buckets with many, many\n",
    "files - using Python generators and very elegant data pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyFunctional for our data pipeline and boto3 for AWS S3 bucket access\n",
    "!pip install pyfunctional boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the boring imports\n",
    "import gzip\n",
    "import json\n",
    "from typing import Any, Generator\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Import the very nice PyFunctional library: https://github.com/EntilZha/PyFunctional\n",
    "# See also: https://pyfunctional.pedro.ai/#transformations-and-actions-apis\n",
    "from functional import seq  # type: ignore[reportMissingTypeStubs]\n",
    "\n",
    "# Import the AWS library boto3: https://pypi.org/project/boto3/\n",
    "# We want to connect to S3 without any authentication, so we also grab the other two imports\n",
    "import boto3  # type: ignore[reportMissingTypeStubs]\n",
    "from botocore import UNSIGNED  # type: ignore[reportMissingTypeStubs]\n",
    "from botocore.client import Config  # type: ignore[reportMissingTypeStubs]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 constants\n",
    "BUCKET_NAME = \"mccarter-blog\"\n",
    "FILE_PREFIX = \"2023-02-18/small-files/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to S3 anonymously - the S3 bucket has permissions set to allow this\n",
    "s3 = boto3.client(\"s3\", config=Config(signature_version=UNSIGNED))  # type: ignore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traditional_approach(\n",
    "        s3,  # type: ignore\n",
    "        bucket_name: str,\n",
    "        prefix: str,\n",
    "    ):\n",
    "    # List the objects in our S3 Bucket\n",
    "    s3_response: dict[str, Any] = s3.list_objects_v2(\n",
    "            Bucket=bucket_name, Prefix=prefix\n",
    "        )  # type: ignore\n",
    "\n",
    "    # Iterate through each object in the response\n",
    "    for obj in s3_response.get(\"Contents\", []):  # type: ignore\n",
    "        # Get the object key (i.e., file path)\n",
    "        key: str = obj.get(\"Key\")\n",
    "\n",
    "        # Skip directories - we only process files directly where we're told to\n",
    "        if key.endswith(\"/\"):\n",
    "            continue\n",
    "\n",
    "        # Download the given file\n",
    "        print(\"Downloading file: \", key)\n",
    "        obj_response = s3.get_object(Bucket=bucket_name, Key=key)  # type: ignore\n",
    "        content: bytes = obj_response[\"Body\"].read()  # type: ignore\n",
    "\n",
    "        # Decompress and parse the JSON .gzip data file\n",
    "        json_data: dict[Any,Any] = json.loads(gzip.decompress(content))\n",
    "\n",
    "        # Process our JSON data\n",
    "        for entry in json_data:\n",
    "            rgb = entry[\"rgb\"]\n",
    "\n",
    "            # Determine if the colour is reddish\n",
    "            is_reddish = rgb[0] > rgb[1] and rgb[0] > rgb[2]\n",
    "            if is_reddish:\n",
    "                print( entry )\n",
    "\n",
    "traditional_approach(s3, BUCKET_NAME, FILE_PREFIX)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our business / domain layer\n",
    "\n",
    "Here we define our simple domain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class ColourId:\n",
    "    \"\"\"A very small data class that holds a colour name and details.\"\"\"\n",
    "\n",
    "    id: str\n",
    "    name: str\n",
    "    base: str\n",
    "    hex: str\n",
    "    rgb: tuple[int, int, int]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some simple questions we can ask of our colour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_colour_reddish(colour: ColourId) -> bool:\n",
    "    return colour.rgb[0] > colour.rgb[1] and colour.rgb[0] > colour.rgb[2]\n",
    "\n",
    "\n",
    "def is_colour_muted(colour: ColourId) -> bool:\n",
    "    return colour.rgb[0] < 128 and colour.rgb[1] < 128 and colour.rgb[2] < 128"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "These are small helper functions.\n",
    "\n",
    "Each function does exactly one task, following the Single Responsibility Principal.\n",
    "\n",
    "We can then easily compose these functions into a simple data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_s3_bucket_file_keys(\n",
    "    s3,  # type: ignore\n",
    "    bucket_name: str,\n",
    "    prefix: str,\n",
    ") -> Generator[str, None, None]:\n",
    "    \"\"\"Returns a generator of file names (technically S3 bucket keys) from the given S3 bucket\n",
    "    that have the given path prefix.\n",
    "\n",
    "    Args:\n",
    "        s3: The boto3 S3 client object\n",
    "        bucket_name (str): The name of the S3 bucket to retrieve data from\n",
    "        prefix (str): The \"file path\" prefix of the files (S3 bucket keys) to be returned.\n",
    "            So all file paths returned from this function will have this prefix.\n",
    "\n",
    "    Yields:\n",
    "        Generator[str, None, None]: A sequence of matching file names (S3 bucket keys).\n",
    "    \"\"\"\n",
    "\n",
    "    # List all objects in the bucket with the specified \"file/folder\" prefix\n",
    "    # See: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/list_objects_v2.html\n",
    "    response: dict[str, Any] = s3.list_objects_v2(\n",
    "        Bucket=bucket_name, Prefix=prefix\n",
    "    )  # type: ignore\n",
    "\n",
    "    # Iterate through each object in the response\n",
    "    for obj in response.get(\"Contents\", []):  # type: ignore\n",
    "        # Get the object key (i.e., file path)\n",
    "        key: str = obj.get(\"Key\")\n",
    "\n",
    "        # Skip directories - this function only returns file paths\n",
    "        if key.endswith(\"/\"):\n",
    "            continue\n",
    "\n",
    "        yield key\n",
    "\n",
    "test_keys = list_s3_bucket_file_keys(s3, BUCKET_NAME, FILE_PREFIX)\n",
    "test_results = seq(test_keys).take(3)  # type: ignore\n",
    "expected = [\n",
    "    \"2023-02-18/small-files/black.json.gz\",\n",
    "    \"2023-02-18/small-files/blue.json.gz\",\n",
    "    \"2023-02-18/small-files/gray.json.gz\",\n",
    "]\n",
    "assert test_results == expected, f\"Expected {expected} but got {test_results}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_s3_file(s3, bucket_name: str, key: str) -> bytes:  # type: ignore\n",
    "    \"\"\"Download the given file (S3 bucket key) from S3 and return the raw bytes\n",
    "\n",
    "    Args:\n",
    "        s3: The boto3 S3 client object\n",
    "        bucket_name (str): The name of the S3 bucket to retrieve data from\n",
    "        key (str): The S3 key (file name if you will) of the object to download\n",
    "\n",
    "    Returns:\n",
    "        bytes: The raw bytes of the given file from S3\n",
    "    \"\"\"\n",
    "    obj_response = s3.get_object(Bucket=bucket_name, Key=key)  # type: ignore\n",
    "    content: bytes = obj_response[\"Body\"].read()  # type: ignore\n",
    "    return content\n",
    "\n",
    "\n",
    "# Do something with the content (e.g., print it)\n",
    "content = download_s3_file(s3, BUCKET_NAME, \"2023-02-18/small-files/blue.json.gz\")\n",
    "assert type(content) == bytes, f\"Expected {type(bytes)} but got {type(content)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_binary_file(file_name: str) -> bytes:\n",
    "    \"\"\"A very simple function that reads the byte for the given file name\n",
    "\n",
    "    Args:\n",
    "        file_name (str): The name of the file to read\n",
    "\n",
    "    Returns:\n",
    "        bytes: The raw bytes of the given binary file\n",
    "    \"\"\"\n",
    "    with open(file_name, \"rb\") as file:  # r for reading, b for binary file\n",
    "        return file.read()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data access layer\n",
    "\n",
    "Our data access layer code is very simle - just a helper function to convert a JSON object\n",
    "into an instance of our `ColourId` domain object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_colour_from_json(data: dict[str, Any]) -> ColourId:\n",
    "    return ColourId(\n",
    "        data[\"id\"],\n",
    "        data[\"name\"],\n",
    "        data[\"base\"],\n",
    "        data[\"hex\"],\n",
    "        data[\"rgb\"],\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The usual imperative way improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 76 colours\n",
      "ColourId(id='b6907772-f286-4df1-b699-1d9dee8fdbb2', name='Asphalt', base='black', hex='#0C0404', rgb=[12, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "result: list[ColourId] = []\n",
    "\n",
    "for s3_key in list_s3_bucket_file_keys(s3, BUCKET_NAME, FILE_PREFIX):  # type: ignore\n",
    "    downloaded_file_bytes = download_s3_file(s3, BUCKET_NAME, s3_key)  # type: ignore\n",
    "    decompressed_text = gzip.decompress(downloaded_file_bytes)\n",
    "    json_data = json.loads(decompressed_text)\n",
    "\n",
    "    for colour_dict in json_data:\n",
    "        colour = create_colour_from_json(colour_dict)\n",
    "\n",
    "        # Apply our filters\n",
    "        if not is_colour_reddish(colour):\n",
    "            continue\n",
    "        if not is_colour_muted(colour):\n",
    "            continue\n",
    "\n",
    "        result.append(colour)\n",
    "\n",
    "print(f\"Found {len(result)} colours\")\n",
    "print(result[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data pipeline examples\n",
    "\n",
    "Our first data pipeline just reads some test files from our local drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 185 colours\n",
      "{'id': '1f1b8535-c724-4fd0-8b39-14af954a6642', 'name': 'Lavender', 'base': 'purple', 'hex': '#A689E1', 'rgb': [166, 137, 225]}\n"
     ]
    }
   ],
   "source": [
    "test_files = [\n",
    "    \"./small-data-files/purple.json.gz\",\n",
    "    \"./small-data-files/gray.json.gz\",\n",
    "    \"./small-data-files/green.json.gz\",\n",
    "]\n",
    "\n",
    "result: list[ColourId] = (  # type: ignore\n",
    "    seq(test_files)  # type: ignore\n",
    "    .map(read_binary_file)\n",
    "    .map(gzip.decompress)  # type: ignore\n",
    "    .map(json.loads)  # type: ignore\n",
    "    .flatten()  # type: ignore\n",
    "    .to_list()  # type: ignore\n",
    ")\n",
    "\n",
    "print(f\"Found {len(result)} colours\")\n",
    "print(result[0])\n",
    "# print(json.dumps(result, indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our second data pipeline reads all of our .gz compressed JSON files from the S3 bucket.\n",
    "\n",
    "We:\n",
    "1. Get the list of all the files in our S3 bucket (prefix) folder \n",
    "2. Download the raw bytes of the file\n",
    "3. Decompress the in-memory bytes to get the original .json file contents\n",
    "4. Parse the in-memory JSON data into a Python list of dictionaries\n",
    "5. Convert each dictionary into our Python `ColourId` domain object\n",
    "6. Then we convert the entire thing into an in-memory list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 76 colours\n",
      "ColourId(id='b6907772-f286-4df1-b699-1d9dee8fdbb2', name='Asphalt', base='black', hex='#0C0404', rgb=[12, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "result: list[ColourId] = (\n",
    "    seq(list_s3_bucket_file_keys(s3, BUCKET_NAME, FILE_PREFIX))\n",
    "    .map(lambda key: download_s3_file(s3, BUCKET_NAME, key))\n",
    "    .map(gzip.decompress)  # type: ignore\n",
    "    .map(json.loads)  # type: ignore\n",
    "    .flatten()  # type: ignore\n",
    "    .map(create_colour_from_json)  # type: ignore\n",
    "    .filter(is_colour_reddish)  # type: ignore\n",
    "    .filter(is_colour_muted)  # type: ignore\n",
    "    .to_list()  # type: ignore\n",
    ")\n",
    "\n",
    "print(f\"Found {len(result)} colours\")\n",
    "print(result[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb06f964ad5f6b1bbfd3fdf7106bdbdd9638f61f30f9c922a33f9fa20d941e6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
